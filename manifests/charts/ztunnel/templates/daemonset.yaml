apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ztunnel
  namespace: {{ .Release.Namespace }}
  labels:
{{- range $key, $val := .Values.ztunnel.deploymentLabels }}
    {{ $key }}: "{{ $val }}"
{{- end }}
spec:
  updateStrategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ztunnel
  template:
    metadata:
      labels:
        sidecar.istio.io/inject: "false"
        app: ztunnel
        ambient-type: ztunnel
        {{- range $key, $val := .Values.ztunnel.podLabels }}
        {{ $key }}: "{{ $val }}"
        {{- end }}
      annotations:
        sidecar.istio.io/inject: "false"
        {{- if .Values.ztunnel.podAnnotations }}
{{ toYaml .Values.ztunnel.podAnnotations | indent 8 }}
        {{- end }}
    spec:
      serviceAccountName: ztunnel
      tolerations:
        - effect: NoSchedule
          operator: Exists
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoExecute
          operator: Exists
      initContainers:
      - name: istio-init
{{- if contains "/" .Values.ztunnel.image }}
        image: "{{ .Values.ztunnel.image }}"
{{- else }}
        {{/*  TODO(https://github.com/istio/istio/issues/43243): use distroless, but we are depending on a variant of things not in the distroless image */}}
        image: "{{ .Values.ztunnel.hub | default .Values.global.hub }}/{{ .Values.ztunnel.image | default "ztunnel" }}:{{ .Values.ztunnel.tag | default .Values.global.tag }}"
{{- end }}
{{- if .Values.global.imagePullPolicy }}
        imagePullPolicy: {{ .Values.global.imagePullPolicy }}
{{- end }}
        securityContext:
          privileged: true
          capabilities:
            add:
            - NET_ADMIN
        command:
          - sh
          - -c
          - |
            set -ex

            PROXY_ORG_SRC_MARK=0x4d2/0xfff
            # tproxy mark, it's only used here.
            MARK=0x400/0xfff
            ORG_SRC_RET_MARK=0x4d3/0xfff

            # Below is from config.sh but used in redirect-worker.sh as well
            POD_OUTBOUND=15001
            POD_INBOUND=15008
            POD_INBOUND_PLAINTEXT=15006

            # socket mark setup
            OUTBOUND_MASK="0x100"
            OUTBOUND_MARK="0x100/$OUTBOUND_MASK"

            SKIP_MASK="0x200"
            SKIP_MARK="0x200/$SKIP_MASK"

            # note!! this includes the skip mark bit, so match on skip mark will match this as well.
            CONNSKIP_MASK="0x220"
            CONNSKIP_MARK="0x220/$CONNSKIP_MASK"

            # note!! this includes the skip mark bit, so match on skip mark will match this as well.
            PROXY_MASK="0x210"
            PROXY_MARK="0x210/$PROXY_MASK"

            PROXY_RET_MASK="0x040"
            PROXY_RET_MARK="0x040/$PROXY_RET_MASK"

            INBOUND_TUN=istioin
            OUTBOUND_TUN=istioout

            # TODO: look into why link local (169.254.x.x) address didn't work
            # they don't respond to ARP.
            INBOUND_TUN_IP=192.168.126.1
            ZTUNNEL_INBOUND_TUN_IP=192.168.126.2
            OUTBOUND_TUN_IP=192.168.127.1
            ZTUNNEL_OUTBOUND_TUN_IP=192.168.127.2
            TUN_PREFIX=30

            # a route table number number we can use to send traffic to envoy (should be unused).
            INBOUND_ROUTE_TABLE=100
            INBOUND_ROUTE_TABLE2=103
            OUTBOUND_ROUTE_TABLE=101
            # needed for original src.
            PROXY_ROUTE_TABLE=102

            set +e # Only for delete, we don't care if this fails
            ip link del p$INBOUND_TUN
            ip link del p$OUTBOUND_TUN
            set -e
{{- if not (.Capabilities.KubeVersion.GitVersion | contains "-eks") }}
            HOST_IP=$(ip route | grep default | awk '{print $3}')
{{- end }}

            ip link add name p$INBOUND_TUN type geneve id 1000 remote $HOST_IP
            ip addr add $ZTUNNEL_INBOUND_TUN_IP/$TUN_PREFIX dev p$INBOUND_TUN

            ip link add name p$OUTBOUND_TUN type geneve id 1001 remote $HOST_IP
            ip addr add $ZTUNNEL_OUTBOUND_TUN_IP/$TUN_PREFIX dev p$OUTBOUND_TUN

            ip link set p$INBOUND_TUN up
            ip link set p$OUTBOUND_TUN up

            echo 0 > /proc/sys/net/ipv4/conf/p$INBOUND_TUN/rp_filter
            echo 0 > /proc/sys/net/ipv4/conf/p$OUTBOUND_TUN/rp_filter

            set +e # Only for delete, we don't care if this fails
            ip rule del priority 20000
            ip rule del priority 20001
            ip rule del priority 20002
            ip rule del priority 20003

            ip route flush table 100
            ip route flush table 101
            ip route flush table 102
            set -e

            ip rule add priority 20000 fwmark $MARK lookup 100
            ip rule add priority 20003 fwmark $ORG_SRC_RET_MARK lookup 100
            ip route add local 0.0.0.0/0 dev lo table 100

            ip route add table 101 $HOST_IP dev eth0 scope link
            ip route add table 101 0.0.0.0/0 via $OUTBOUND_TUN_IP dev p$OUTBOUND_TUN

            ip route add table 102 $HOST_IP dev eth0 scope link
            ip route add table 102 0.0.0.0/0 via $INBOUND_TUN_IP dev p$INBOUND_TUN

            set +e
            num_legacy_lines=$( (iptables-legacy-save || true; ip6tables-legacy-save || true) 2>/dev/null | grep '^-' | wc -l)
            if [ "${num_legacy_lines}" -ge 10 ]; then
              mode=legacy
            else
              num_nft_lines=$( (timeout 5 sh -c "iptables-nft-save; ip6tables-nft-save" || true) 2>/dev/null | grep '^-' | wc -l)
              if [ "${num_legacy_lines}" -ge "${num_nft_lines}" ]; then
                mode=legacy
              else
                mode=nft
              fi
            fi
            IPTABLES=iptables-legacy
            if [ "${mode}" = "nft" ]; then
              IPTABLES=iptables-nft
            fi
            set -e

            $IPTABLES -t mangle -F PREROUTING
            $IPTABLES -t nat -F OUTPUT

            $IPTABLES -t mangle -A PREROUTING -p tcp -i p$INBOUND_TUN -m tcp --dport=$POD_INBOUND -j TPROXY --tproxy-mark $MARK --on-port $POD_INBOUND --on-ip 127.0.0.1
            $IPTABLES -t mangle -A PREROUTING -p tcp -i p$OUTBOUND_TUN -j TPROXY --tproxy-mark $MARK --on-port $POD_OUTBOUND --on-ip 127.0.0.1
            $IPTABLES -t mangle -A PREROUTING -p tcp -i p$INBOUND_TUN -j TPROXY --tproxy-mark $MARK --on-port $POD_INBOUND_PLAINTEXT --on-ip 127.0.0.1

            $IPTABLES -t mangle -A PREROUTING -p tcp -i eth0 ! --dst $INSTANCE_IP -j MARK --set-mark $ORG_SRC_RET_MARK
            # With normal linux routing we need to disable the rp_filter
            # as we get packets from a tunnel that doesn't have default routes.
            echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter
            echo 0 > /proc/sys/net/ipv4/conf/default/rp_filter
            echo 0 > /proc/sys/net/ipv4/conf/eth0/rp_filter
        env:
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        {{- if .Values.meshConfig.defaultConfig.proxyMetadata }}
        {{- range $key, $value := .Values.meshConfig.defaultConfig.proxyMetadata}}
        - name: {{ $key }}
          value: "{{ $value }}"
        {{- end }}
        {{- end }}
        {{- if .Values.ztunnel.env }}
        {{- range $key, $val := .Values.ztunnel.env }}
        - name: {{ $key }}
          value: "{{ $val }}"
        {{- end }}
        {{- end }}
      containers:
      - name: istio-proxy
{{- if contains "/" .Values.ztunnel.image }}
        image: "{{ .Values.ztunnel.image }}"
{{- else }}
        image: "{{ .Values.ztunnel.hub | default .Values.global.hub }}/{{ .Values.ztunnel.image | default "proxyv2" }}:{{ .Values.ztunnel.tag | default .Values.global.tag }}{{with (.Values.ztunnel.variant | default .Values.global.variant)}}-{{.}}{{end}}"
{{- end }}
{{- if .Values.global.imagePullPolicy }}
        imagePullPolicy: {{ .Values.global.imagePullPolicy }}
{{- end }}
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
        readinessProbe:
          httpGet:
            port: 15021
            path: /healthz/ready
        args:
        - proxy
        - ztunnel
        env:
        - name: CLUSTER_ID
          value: {{ .Values.global.multiCluster.clusterName | default "Kubernetes" }}
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: INSTANCE_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: SERVICE_ACCOUNT
          valueFrom:
            fieldRef:
              fieldPath: spec.serviceAccountName
        {{- if .Values.meshConfig.defaultConfig.proxyMetadata }}
        {{- range $key, $value := .Values.meshConfig.defaultConfig.proxyMetadata}}
        - name: {{ $key }}
          value: "{{ $value }}"
        {{- end }}
        {{- end }}
        {{- if .Values.ztunnel.env }}
        {{- range $key, $val := .Values.ztunnel.env }}
        - name: {{ $key }}
          value: "{{ $val }}"
        {{- end }}
        {{- end }}
        volumeMounts:
        - mountPath: /var/run/secrets/istio
          name: istiod-ca-cert
        - mountPath: /var/run/secrets/tokens
          name: istio-token
        {{- if .Values.spireConfig.enable }}  
        - mountPath: {{ .Values.spireConfig.socketPath }}
          name: workload-socket
          readOnly: true
        {{- end}}  
      volumes:
      - name: istio-token
        projected:
          sources:
          - serviceAccountToken:
              path: istio-token
              expirationSeconds: 43200
              audience: istio-ca
      - name: istiod-ca-cert
        configMap:
          name: istio-ca-root-cert
      {{- if .Values.spireConfig.enable }}    
      - name: workload-socket
        csi:
          driver: "csi.spiffe.io"
      {{- end}}    